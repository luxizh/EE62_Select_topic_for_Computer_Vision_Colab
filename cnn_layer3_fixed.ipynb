{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_layer3_fixed.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luxizh/EE62_Select_topic_for_Computer_Vision_Colab/blob/master/cnn_layer3_fixed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zHXM-_pm8f4i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import pickle\n",
        "from google.colab import files\n",
        "tf.reset_default_graph()\n",
        "import time\n",
        "\n",
        "##################################################################\n",
        "batch_size = 128\n",
        "z_dim = 100\n",
        "learning_rate_d = 0.001\n",
        "learning_rate_g = 0.0002\n",
        "image_width = 32\n",
        "image_height = 32\n",
        "ndf = 16\n",
        "ngf = 16\n",
        "beta1 = 0.5\n",
        "beta2 = 0.9\n",
        "max_iter_step = 20000\n",
        "channels = 1\n",
        "log_path = './log_cgan'\n",
        "ckpt_path = './ckpt_cgan'\n",
        "ckpt_step_path = ckpt_path + '.step'\n",
        "dataset = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
        "epochs=20\n",
        "##################################################################\n",
        "\n",
        "\n",
        "def get_batches():\n",
        "    X, y = dataset.train.next_batch(batch_size)\n",
        "    X = 2 * X - 1\n",
        "    X = np.reshape(X, (-1, 28, 28))\n",
        "    X = np.pad(X, pad_width=((0, 0), (2, 2), (2, 2)),\n",
        "               mode='constant', constant_values=-1)\n",
        "    X = np.expand_dims(X, -1)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def leaky_relu(x, leak=0.2, name='leaky_relu'):\n",
        "    with tf.variable_scope(name):\n",
        "        f1 = 0.5 * (1 + leak)\n",
        "        f2 = 0.5 * (1 - leak)\n",
        "        return f1 * x + f2 * tf.abs(x)\n",
        "\n",
        "\n",
        "def conv_cond_concat(x, y):\n",
        "    x_shapes = x.get_shape()\n",
        "    y_shapes = y.get_shape()\n",
        "\n",
        "    return tf.concat([x, y * tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[3]])], 3)\n",
        "\n",
        "\n",
        "def generator(z, y, channels, training=True):\n",
        "    with tf.variable_scope(\"generator\", reuse=(not training)):\n",
        "        # y_ = tf.reshape(y, shape=[batch_size, 1, 1, 10])\n",
        "        # 8*8*128\n",
        "        z = tf.concat([z, y], axis=1)\n",
        "        x = tf.layers.dense(z, 8 * 8 * ngf * 8)\n",
        "        #8*8*128\n",
        "        deconv1 = tf.reshape(x, (-1, 8, 8, ngf * 8))\n",
        "        bn1 = tf.layers.batch_normalization(deconv1, training=training)\n",
        "        relu1 = tf.nn.relu(bn1)\n",
        "        # out1 = conv_cond_concat(relu1, y_)\n",
        "        \n",
        "        #16*16*64\n",
        "        deconv2 = tf.layers.conv2d_transpose(\n",
        "            relu1, ngf * 4, 3, strides=2, padding='SAME')\n",
        "        bn2 = tf.layers.batch_normalization(deconv2, training=training)\n",
        "        relu2 = tf.nn.relu(bn2)\n",
        "        # out2 = conv_cond_concat(relu2, y_)\n",
        "        \n",
        "        #32*32*32\n",
        "        deconv3 = tf.layers.conv2d_transpose(\n",
        "            relu2, ngf * 2, 3, strides=2, padding='SAME')\n",
        "        bn3 = tf.layers.batch_normalization(deconv3, training=training)\n",
        "        relu3 = tf.nn.relu(bn3)\n",
        "        # out3 = conv_cond_concat(relu3, y_)\n",
        "        \n",
        "        #32*32*16\n",
        "        #deconv4 = tf.layers.conv2d_transpose(\n",
        "        #    relu3, ngf, 3, strides=2, padding='SAME')\n",
        "       # bn4 = tf.layers.batch_normalization(deconv4, training=training)\n",
        "        #relu4 = tf.nn.relu(bn4)\n",
        "        # out4 = conv_cond_concat(relu4, y_)\n",
        "\n",
        "        #32*32*1\n",
        "        deconv4 = tf.layers.conv2d_transpose(\n",
        "            relu3, channels, 3, strides=1, padding='SAME')\n",
        "        out = tf.nn.tanh(deconv4)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def discriminator(image, y, reuse=False):\n",
        "    with tf.variable_scope('discriminator', reuse=reuse):\n",
        "        y_ = tf.reshape(y, shape=(batch_size, 1, 1, 10))\n",
        "        image = conv_cond_concat(image, y_)\n",
        "\n",
        "        #conv1 = tf.layers.conv2d(image, ndf, 3, strides=2, padding='SAME')\n",
        "        #lrelu1 = leaky_relu(conv1)\n",
        "        # out1 = conv_cond_concat(lrelu1, y_)\n",
        "\n",
        "        conv2 = tf.layers.conv2d(image, ndf * 2, 3, strides=2, padding='SAME')\n",
        "        #bn2 = tf.layers.batch_normalization(conv2, training=True)\n",
        "        lrelu2 = leaky_relu(conv2)\n",
        "        # out2 = conv_cond_concat(lrelu2, y_)\n",
        "\n",
        "        conv3 = tf.layers.conv2d(lrelu2, ndf * 4, 3, strides=2, padding='SAME')\n",
        "        bn3 = tf.layers.batch_normalization(conv3, training=True)\n",
        "        lrelu3 = leaky_relu(bn3)\n",
        "        # out3 = conv_cond_concat(lrelu3, y_)\n",
        "\n",
        "        conv4 = tf.layers.conv2d(lrelu3, ndf * 8, 3, strides=1, padding='SAME')\n",
        "        bn4 = tf.layers.batch_normalization(conv4, training=True)\n",
        "        lrelu4 = leaky_relu(bn4)\n",
        "        # out4 = conv_cond_concat(lrelu4, y_)\n",
        "\n",
        "        flat = tf.reshape(lrelu4, [batch_size, -1])\n",
        "\n",
        "        # flat = tf.concat([flat, y], axis=1)\n",
        "\n",
        "        logits = tf.layers.dense(flat, 1)\n",
        "\n",
        "        out = tf.sigmoid(logits)\n",
        "\n",
        "        return out, logits\n",
        "\n",
        "\n",
        "def model_inputs():\n",
        "    inputs_real = tf.placeholder(tf.float32, shape=(\n",
        "        batch_size, image_width, image_height, channels))\n",
        "    inputs_y = tf.placeholder(tf.float32, shape=(batch_size, 10))\n",
        "    inputs_z = tf.placeholder(tf.float32, shape=(batch_size, z_dim))\n",
        "\n",
        "    return inputs_real, inputs_y, inputs_z\n",
        "\n",
        "\n",
        "def model_loss(input_real, input_y, input_z):\n",
        "    label_smoothing = 0.9\n",
        "\n",
        "    g = generator(input_z, input_y, channels)\n",
        "    d_real, d_logits_real = discriminator(input_real, input_y)\n",
        "    d_fake, d_logits_fake = discriminator(g, input_y, reuse=True)\n",
        "\n",
        "    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "        logits=d_logits_real, labels=tf.ones_like(d_real) * label_smoothing))\n",
        "    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "        logits=d_logits_fake, labels=tf.zeros_like(d_fake) * label_smoothing))\n",
        "\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "        logits=d_logits_fake, labels=tf.ones_like(d_fake) * label_smoothing))\n",
        "\n",
        "    return d_loss, g_loss\n",
        "\n",
        "\n",
        "def model_opt(d_loss, g_loss):\n",
        "    d_vars = tf.get_collection(\n",
        "        tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
        "    g_vars = tf.get_collection(\n",
        "        tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
        "\n",
        "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
        "        d_opt = tf.train.AdamOptimizer(\n",
        "            learning_rate_d, beta1, beta2).minimize(d_loss, var_list=d_vars)\n",
        "        g_opt = tf.train.AdamOptimizer(\n",
        "            learning_rate_g, beta1, beta2).minimize(g_loss, var_list=g_vars)\n",
        "\n",
        "    return d_opt, g_opt\n",
        "\n",
        "\n",
        "def main():\n",
        "    input_real, input_y, input_z = model_inputs()\n",
        "    d_loss, g_loss = model_loss(input_real, input_y, input_z)\n",
        "    d_opt, g_opt = model_opt(d_loss, g_loss)\n",
        "\n",
        "    d_loss_sum = tf.summary.scalar(\"d_loss\", d_loss)\n",
        "    g_loss_sum = tf.summary.scalar(\"g_loss\", g_loss)\n",
        "    merged_all = tf.summary.merge_all()\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()\n",
        "    summary_writer = tf.summary.FileWriter(log_path, tf.get_default_graph())\n",
        "\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
        "\n",
        "    fixed_z = np.random.normal(0.0, 1.0, size=(batch_size, z_dim))\n",
        "\n",
        "    with tf.Session(config=config) as sess:\n",
        "        Epoch_D_train_losses=[]\n",
        "        Epoch_G_train_losses=[]\n",
        "        '''\n",
        "        if os.path.isfile(ckpt_step_path):\n",
        "            with open(ckpt_step_path, 'rb') as f:\n",
        "                start_step = int(f.read())\n",
        "            print('Training was interrupted. Continuing at step', start_step)\n",
        "            saver.restore(sess, ckpt_path)\n",
        "        else:\n",
        "            start_step = 0\n",
        "            sess.run(init)\n",
        "        '''\n",
        "        sess.run(init)\n",
        "        step=0\n",
        "        #sample_image_id = 0\n",
        "        for epoch in range(epochs):\n",
        "            D_train_losses=[]\n",
        "            G_train_losses=[]\n",
        "            for i in range(dataset.train.num_examples //batch_size):\n",
        "                step+=1\n",
        "                x, y = get_batches()\n",
        "                z = np.random.normal(0.0, 1.0, size=(batch_size, z_dim))\n",
        "                sess.run(d_opt, feed_dict={input_real: x, input_y: y, input_z: z})\n",
        "                sess.run(g_opt, feed_dict={input_real: x, input_y: y, input_z: z})\n",
        "\n",
        "                if step % 50 == 0:\n",
        "                    d_loss_val, g_loss_val, merged_summary = sess.run([d_loss, g_loss, merged_all],\n",
        "                                                                    feed_dict={input_real: x, input_y: y, input_z: z})\n",
        "                    print('epoch: %d/%d step: %d d_loss: %f, g_loss: %f' %\n",
        "                        (epoch+1, epochs, step, d_loss_val, g_loss_val))\n",
        "                    summary_writer.add_summary(merged_summary, step)\n",
        "                    D_train_losses.append(d_loss_val)\n",
        "                    G_train_losses.append(g_loss_val)\n",
        "\n",
        "\n",
        "\n",
        "            if (step != 0 and step % 500 == 0):\n",
        "                saver.save(sess, ckpt_path)\n",
        "                print('Save model at step', step)\n",
        "                with open(ckpt_step_path, 'wb') as f:\n",
        "                    f.write(b'%d' % (step + 1))\n",
        "            \n",
        "            overall = []\n",
        "            G_y = tf.placeholder(tf.float32, shape=(None, 10))\n",
        "            G_z = tf.placeholder(tf.float32, shape=(None, z_dim))\n",
        "            g = generator(G_z, G_y, channels, training=False)\n",
        "            for r in range(0, 10):\n",
        "                y = np.zeros((batch_size, 10))\n",
        "                y[:, r] = 1\n",
        "                gen_images = g.eval(\n",
        "                    feed_dict={G_z: fixed_z, G_y: y})\n",
        "                temp = []\n",
        "                for c in range(10):\n",
        "                    temp.append(gen_images[c])\n",
        "                overall.append(np.concatenate(temp, axis=1))\n",
        "            res = np.concatenate(overall, axis=0)\n",
        "            res = np.squeeze(res)\n",
        "            plt.figure(figsize=[10, 10])\n",
        "            plt.axis('off')\n",
        "            res = 1 - (res + 1) / 2\n",
        "            plt.imshow(res, cmap='binary')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            #plt.savefig('./result/%d.png' % sample_image_id, format='png', dpi=51.2)\n",
        "            #print('Saving %d.png' % sample_image_id)\n",
        "            plt.close('all')\n",
        "            #sample_image_id += 1\n",
        "            Epoch_D_train_losses.append(np.mean(D_train_losses))\n",
        "            Epoch_G_train_losses.append(np.mean(G_train_losses))\n",
        "            print('epoch: %d/%d mean d_loss: %f, g_loss: %f' %\n",
        "                (epoch+1, epochs, np.mean(D_train_losses), np.mean(G_train_losses)))\n",
        "        test_size=1000\n",
        "        test_z = np.random.normal(0.0, 1.0, size=(test_size*10, z_dim)) \n",
        "        y_test = np.zeros((test_size*10, 10))\n",
        "        test_label=np.zeros([test_size*10])\n",
        "        for r in range(0, 10):\n",
        "            y_test[r*test_size:(r+1)*test_size, r] = 1\n",
        "            test_label[r*test_size:(r+1)*test_size]=r\n",
        "        test_images = g.eval(\n",
        "            feed_dict={G_z: test_z, G_y: y_test})\n",
        "        print('images gengerated')\n",
        "        filename1 = 'test_images.data'\n",
        "        f = open(filename1, 'wb')\n",
        "        pickle.dump(test_images,f)\n",
        "        print(test_images.shape)\n",
        "        f.close()\n",
        "\n",
        "\n",
        "        filename2 = 'test_label.data'\n",
        "        f = open(filename2, 'wb')\n",
        "        pickle.dump(test_label,f)\n",
        "        #test_images.shape()\n",
        "        f.close()\n",
        "                #time.sleep(5)\n",
        "                #files.download(filename1)\n",
        "                #files.download(filename2)\n",
        "                \n",
        "        #os.remove(ckpt_step_path)\n",
        "    return Epoch_D_train_losses,Epoch_G_train_losses,test_images,test_label\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Epoch_D_train_losses,Epoch_G_train_losses,test_images,test_label=main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g6Q76MZn8pt8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = np.linspace(1, epochs, epochs)\n",
        "fig = plt.figure()\n",
        "plt.plot(x, Epoch_D_train_losses,label='D_loss' )\n",
        "plt.plot(x, Epoch_G_train_losses, label='G_loss' )\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc=1)\n",
        "plt.ylabel('Loss')\n",
        "print(Epoch_D_train_losses)\n",
        "print(Epoch_G_train_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F3Pw5MEw8sxz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "print(Epoch_D_train_losses)\n",
        "print(Epoch_G_train_losses)\n",
        "filename = 'Dloss.data'\n",
        "f = open(filename, 'wb')\n",
        "pickle.dump(Epoch_D_train_losses,f)\n",
        "f.close()\n",
        "filename = 'Gloss.data'\n",
        "f = open(filename, 'wb')\n",
        "pickle.dump(Epoch_G_train_losses,f)\n",
        "f.close()\n",
        "from google.colab import files\n",
        "files.download('Dloss.data')\n",
        "files.download('Gloss.data')\n",
        "\n",
        "files.download(filename1)\n",
        "files.download(filename2)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}